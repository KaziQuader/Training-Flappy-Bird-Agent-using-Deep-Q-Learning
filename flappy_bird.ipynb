{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "117bae1b-069d-455a-b110-dac8a8dee356",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /opt/homebrew/lib/python3.11/site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /opt/homebrew/lib/python3.11/site-packages (from gym) (1.25.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/homebrew/lib/python3.11/site-packages (from gym) (3.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /opt/homebrew/lib/python3.11/site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: tensorflow in /opt/homebrew/lib/python3.11/site-packages (2.15.0)\n",
      "Requirement already satisfied: tensorflow-macos==2.15.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.25.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (4.23.4)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (4.8.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.34.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.60.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/homebrew/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.15.0->tensorflow) (0.41.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/homebrew/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.25.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/homebrew/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (1.1.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/homebrew/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.5.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/homebrew/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/homebrew/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/homebrew/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/homebrew/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/homebrew/lib/python3.11/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/homebrew/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/homebrew/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/homebrew/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: utils in /opt/homebrew/lib/python3.11/site-packages (1.0.1)\n",
      "Requirement already satisfied: pyvirtualdisplay in /opt/homebrew/lib/python3.11/site-packages (3.0)\n",
      "Requirement already satisfied: flappy-bird-gymnasium in /opt/homebrew/lib/python3.11/site-packages (0.3.0)\n",
      "Requirement already satisfied: gymnasium in /opt/homebrew/lib/python3.11/site-packages (from flappy-bird-gymnasium) (0.29.1)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.11/site-packages (from flappy-bird-gymnasium) (1.25.2)\n",
      "Requirement already satisfied: pygame in /opt/homebrew/lib/python3.11/site-packages (from flappy-bird-gymnasium) (2.5.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/homebrew/lib/python3.11/site-packages (from gymnasium->flappy-bird-gymnasium) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/homebrew/lib/python3.11/site-packages (from gymnasium->flappy-bird-gymnasium) (4.8.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/homebrew/lib/python3.11/site-packages (from gymnasium->flappy-bird-gymnasium) (0.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym\n",
    "!pip install tensorflow\n",
    "!pip install utils\n",
    "!pip install pyvirtualdisplay\n",
    "!pip install flappy-bird-gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe7d9d8-d379-409e-986b-64e174826889",
   "metadata": {},
   "source": [
    "### Importing Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7f9d4d2-b7b1-4969-ac07-03c4e788ca84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import deque, namedtuple\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import utils\n",
    "import random\n",
    "import flappy_bird_gymnasium\n",
    "import gymnasium\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.losses import MSE\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deafaef-d7f9-41a9-a3c0-42d685275204",
   "metadata": {},
   "source": [
    "### The Flappy Bird Environment\n",
    "- Action Space\n",
    "  - do nothing = 0\n",
    "  - Flap = 1\n",
    "- Observation Space\n",
    "  - the last pipe's horizontal position\n",
    "  - the last top pipe's vertical position\n",
    "  - the last bottom pipe's vertical position\n",
    "  - the next pipe's horizontal position\n",
    "  - the next top pipe's vertical position\n",
    "  - the next bottom pipe's vertical position\n",
    "  - the next next pipe's horizontal position\n",
    "  - the next next top pipe's vertical position\n",
    "  - the next next bottom pipe's vertical position\n",
    "  - player's vertical position\n",
    "  - player's vertical velocity\n",
    "  - player's rotation\n",
    "- Rewards\n",
    "  - +1 every time you pass a pipe\n",
    "  - +0.1 for each frame where you don't collide against the top and bottom bounds\n",
    "  - -1 for dying"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b6db9c-e3c7-4638-b636-4d82b8831677",
   "metadata": {},
   "source": [
    "### Load the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25839b89-f395-4a22-9479-e9bf80a9475c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Shape: 12\n",
      "Number of actions: 2\n"
     ]
    }
   ],
   "source": [
    "env = gymnasium.make(\"FlappyBird-v0\", render_mode=\"human\")\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "print('State Shape:', state_size)\n",
    "print('Number of actions:', num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c131421-9833-4bca-b6f1-c9675ef60931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_state = env.reset()\n",
    "\n",
    "# # Select an action\n",
    "# action = 0\n",
    "\n",
    "# print(\"Initial State:\", initial_state[0])\n",
    "# print(\"Next State:\", env.step(action)[0])\n",
    "# print(\"Reward:\", env.step(action)[1])\n",
    "# print(\"Episode Terminated:\", env.step(action)[2])\n",
    "# print(\"_:\", env.step(action)[2])\n",
    "# print(\"Current Score:\", env.step(action)[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17705a9-12f7-4d40-a9f2-d1aac4df27eb",
   "metadata": {},
   "source": [
    "### Training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a268a28-85bf-4024-af91-86b2e1118f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "MEMORY_SIZE = 100_000     # size of memory buffer\n",
    "GAMMA = 0.995             # discount factor\n",
    "ALPHA = 1e-3              # learning rate  \n",
    "NUM_STEPS_FOR_UPDATE = 1  # perform a learning update every C time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bec1c845-1e7e-4048-93ba-af75f4ced65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "SEED = 0              # seed for pseudo-random number generator\n",
    "MINIBATCH_SIZE = 64   # mini-batch size\n",
    "TAU = 1e-3            # soft update parameter\n",
    "E_DECAY = 0.9       # ε decay rate for ε-greedy policy\n",
    "E_MIN = 0.01          # minimum ε value for ε-greedy policy\n",
    "\n",
    "random.seed(SEED)\n",
    "\n",
    "def get_action(model, state, num_actions, epsilon=0):\n",
    "    # if random.random() > epsilon:\n",
    "    #     return np.argmax(q_values.numpy()[0])\n",
    "    # else:\n",
    "    #     return random.choice([0, 1])\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return np.random.choice(num_actions)\n",
    "    else:\n",
    "        q_values = model.predict(state)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "def check_update_conditions(t, num_steps_upd, memory_buffer):\n",
    "    if (t + 1) % num_steps_upd == 0 and len(memory_buffer) > MINIBATCH_SIZE:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def get_experiences(memory_buffer):\n",
    "    experiences = random.sample(memory_buffer, k=MINIBATCH_SIZE)\n",
    "    states = tf.convert_to_tensor(np.array([e.state for e in experiences if e is not None]),dtype=tf.float32)\n",
    "    actions = tf.convert_to_tensor(np.array([e.action for e in experiences if e is not None]), dtype=tf.float32)\n",
    "    rewards = tf.convert_to_tensor(np.array([e.reward for e in experiences if e is not None]), dtype=tf.float32)\n",
    "    next_states = tf.convert_to_tensor(np.array([e.next_state for e in experiences if e is not None]),dtype=tf.float32)\n",
    "    done_vals = tf.convert_to_tensor(np.array([e.done for e in experiences if e is not None]).astype(np.uint8),\n",
    "                                     dtype=tf.float32)\n",
    "    return (states, actions, rewards, next_states, done_vals)\n",
    "\n",
    "def update_target_network(q_network, target_q_network):\n",
    "    for target_weights, q_net_weights in zip(target_q_network.weights, q_network.weights):\n",
    "        target_weights.assign(TAU * q_net_weights + (1.0 - TAU) * target_weights)\n",
    "\n",
    "def get_new_eps(epsilon):\n",
    "    return max(E_MIN, E_DECAY*epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78489af9-f9ff-4391-8be3-8effc08a48d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store experiences as named tuples\n",
    "experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a855741-0107-4597-b84f-4aec551f95e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "# Create the Q-Network\n",
    "q_network = Sequential([\n",
    "    Input(shape=state_size),                      \n",
    "    Dense(units=64, activation='relu'),            \n",
    "    Dense(units=64, activation='relu'),            \n",
    "    Dense(units=num_actions, activation='linear'),\n",
    "    ])\n",
    "\n",
    "# Create the target Q^-Network\n",
    "target_q_network = Sequential([\n",
    "    Input(shape=state_size),                      \n",
    "    Dense(units=64, activation='relu'),            \n",
    "    Dense(units=64, activation='relu'),            \n",
    "    Dense(units=num_actions, activation='linear'),\n",
    "    ])\n",
    "\n",
    "optimizer = Adam(learning_rate=ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcb5f0d7-bb42-49d2-a45e-b2ca1fd62cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(experiences, gamma, q_network, target_q_network):\n",
    "    \"\"\" \n",
    "    Calculates the loss.\n",
    "    \n",
    "    Args:\n",
    "      experiences: (tuple) tuple of [\"state\", \"action\", \"reward\", \"next_state\", \"done\"] namedtuples\n",
    "      gamma: (float) The discount factor.\n",
    "      q_network: (tf.keras.Sequential) Keras model for predicting the q_values\n",
    "      target_q_network: (tf.keras.Sequential) Karas model for predicting the targets\n",
    "          \n",
    "    Returns:\n",
    "      loss: (TensorFlow Tensor(shape=(0,), dtype=int32)) the Mean-Squared Error between\n",
    "            the y targets and the Q(s,a) values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Unpack the mini-batch of experience tuples\n",
    "    states, actions, rewards, next_states, done_vals = experiences\n",
    "    \n",
    "    # Compute max Q^(s,a)\n",
    "    max_qsa = tf.reduce_max(target_q_network(next_states), axis=-1)\n",
    "    \n",
    "    # Set y = R if episode terminates, otherwise set y = R + γ max Q^(s,a).\n",
    "    y_targets = rewards + (gamma * max_qsa * (1 - done_vals))\n",
    "    \n",
    "    # Get the q_values\n",
    "    q_values = q_network(states)\n",
    "    q_values = tf.gather_nd(q_values, tf.stack([tf.range(q_values.shape[0]),\n",
    "                                                tf.cast(actions, tf.int32)], axis=1))\n",
    "        \n",
    "    # Compute the loss\n",
    "    loss = MSE(y_targets, q_values)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d459a4a8-af64-41f6-8418-909131d9d5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def agent_learn(experiences, gamma):\n",
    "    \"\"\"\n",
    "    Updates the weights of the Q networks.\n",
    "    \n",
    "    Args:\n",
    "      experiences: (tuple) tuple of [\"state\", \"action\", \"reward\", \"next_state\", \"done\"] namedtuples\n",
    "      gamma: (float) The discount factor.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the loss\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(experiences, gamma, q_network, target_q_network)\n",
    "\n",
    "    # Get the gradients of the loss with respect to the weights.\n",
    "    gradients = tape.gradient(loss, q_network.trainable_variables)\n",
    "    \n",
    "    # Update the weights of the q_network.\n",
    "    optimizer.apply_gradients(zip(gradients, q_network.trainable_variables))\n",
    "\n",
    "    # update the weights of target q_network\n",
    "    update_target_network(q_network, target_q_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b614c5ca-e1c3-480c-93e7-63eb9bc62a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[ 0.57821953 -0.04987197]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "False\n",
      "tf.Tensor([[ 0.57072735 -0.04308057]], shape=(1, 2), dtype=float32)\n",
      "1\n",
      "False\n",
      "tf.Tensor([[ 0.58511865 -0.04958145]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "False\n",
      "tf.Tensor([[ 0.57699734 -0.04223641]], shape=(1, 2), dtype=float32)\n",
      "1\n",
      "False\n",
      "tf.Tensor([[ 0.5920201  -0.04929708]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "False\n",
      "tf.Tensor([[ 0.58328754 -0.04141028]], shape=(1, 2), dtype=float32)\n",
      "1\n",
      "False\n",
      "tf.Tensor([[ 0.59892154 -0.04901268]], shape=(1, 2), dtype=float32)\n",
      "1\n",
      "False\n",
      "tf.Tensor([[ 0.60257524 -0.04886209]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "False\n",
      "tf.Tensor([[ 0.5940773  -0.04130581]], shape=(1, 2), dtype=float32)\n",
      "1\n",
      "False\n",
      "tf.Tensor([[ 0.6097695  -0.04899026]], shape=(1, 2), dtype=float32)\n",
      "1\n",
      "False\n",
      "tf.Tensor([[ 0.6137218  -0.04925984]], shape=(1, 2), dtype=float32)\n",
      "1\n",
      "False\n",
      "tf.Tensor([[ 0.6176738  -0.04952943]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "False\n",
      "tf.Tensor([[ 0.6094464  -0.04235423]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "False\n",
      "tf.Tensor([[ 0.6044486  -0.03840028]], shape=(1, 2), dtype=float32)\n",
      "1\n",
      "False\n",
      "tf.Tensor([[ 0.62965596 -0.05087436]], shape=(1, 2), dtype=float32)\n",
      "1\n",
      "False\n",
      "tf.Tensor([[ 0.6343146  -0.05140018]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "False\n",
      "tf.Tensor([[ 0.6253306  -0.04296036]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "False\n",
      "tf.Tensor([[ 0.6186696  -0.03803989]], shape=(1, 2), dtype=float32)\n",
      "1\n",
      "False\n",
      "tf.Tensor([[ 0.64673793 -0.05280219]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "False\n",
      "tf.Tensor([[ 0.6374401  -0.04324831]], shape=(1, 2), dtype=float32)\n",
      "1\n",
      "False\n",
      "tf.Tensor([[ 0.65553784 -0.05379529]], shape=(1, 2), dtype=float32)\n",
      "1\n",
      "False\n",
      "tf.Tensor([[ 0.66019654 -0.05432104]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "False\n",
      "tf.Tensor([[ 0.6507944  -0.04439684]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "False\n",
      "tf.Tensor([[ 0.641832   -0.03680196]], shape=(1, 2), dtype=float32)\n",
      "1\n",
      "False\n",
      "tf.Tensor([[ 0.67225444 -0.05538696]], shape=(1, 2), dtype=float32)\n",
      "1\n",
      "False\n",
      "tf.Tensor([[ 0.6764392  -0.05547645]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "False\n",
      "tf.Tensor([[ 0.666842   -0.04537283]], shape=(1, 2), dtype=float32)\n",
      "1\n",
      "False\n",
      "tf.Tensor([[ 0.6843435  -0.05564558]], shape=(1, 2), dtype=float32)\n",
      "1\n",
      "False\n",
      "tf.Tensor([[ 0.68852824 -0.05573507]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "False\n",
      "tf.Tensor([[ 0.67893106 -0.04563143]], shape=(1, 2), dtype=float32)\n",
      "1\n",
      "False\n",
      "tf.Tensor([[ 0.69643235 -0.05590418]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "False\n",
      "tf.Tensor([[ 0.6868354  -0.04580052]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "False\n",
      "tf.Tensor([[ 0.6767731  -0.03568681]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "False\n",
      "tf.Tensor([[ 0.6692918  -0.02941327]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "False\n",
      "tf.Tensor([[ 0.66287947 -0.02650472]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "False\n",
      "tf.Tensor([[ 0.65336674 -0.02476319]], shape=(1, 2), dtype=float32)\n",
      "1\n",
      "False\n",
      "tf.Tensor([[ 0.71467245 -0.05639378]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "False\n",
      "tf.Tensor([[ 0.7049687  -0.04618831]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "False\n",
      "tf.Tensor([[ 0.6949067  -0.03607472]], shape=(1, 2), dtype=float32)\n",
      "1\n",
      "False\n",
      "tf.Tensor([[ 0.6862804  -0.02768371]], shape=(1, 2), dtype=float32)\n",
      "1\n",
      "False\n",
      "tf.Tensor([[ 0.67812777 -0.0252784 ]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "False\n",
      "tf.Tensor([[ 0.6642063  -0.02164399]], shape=(1, 2), dtype=float32)\n",
      "1\n",
      "False\n",
      "tf.Tensor([[ 0.6481969  -0.01796985]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "False\n",
      "tf.Tensor([[ 0.6287296  -0.01651581]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "False\n",
      "tf.Tensor([[ 0.61034167 -0.01523297]], shape=(1, 2), dtype=float32)\n",
      "1\n",
      "False\n",
      "tf.Tensor([[ 0.591037   -0.00692593]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "False\n",
      "tf.Tensor([[ 0.56985354 -0.00148214]], shape=(1, 2), dtype=float32)\n",
      "1\n",
      "False\n",
      "tf.Tensor([[ 0.55374753 -0.00246322]], shape=(1, 2), dtype=float32)\n",
      "1\n",
      "False\n",
      "tf.Tensor([[ 0.5357621  -0.00800917]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "False\n",
      "tf.Tensor([[ 0.5172611 -0.0107255]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "False\n",
      "tf.Tensor([[ 0.5250587  -0.07046854]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "False\n",
      "tf.Tensor([[ 0.5007243  -0.06539998]], shape=(1, 2), dtype=float32)\n",
      "1\n",
      "False\n",
      "tf.Tensor([[ 0.47361594 -0.05977522]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "False\n",
      "tf.Tensor([[ 0.4416519 -0.0536749]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "False\n",
      "tf.Tensor([[ 0.41206276 -0.05026593]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "False\n",
      "tf.Tensor([[ 0.38659337 -0.04955179]], shape=(1, 2), dtype=float32)\n",
      "1\n",
      "False\n",
      "tf.Tensor([[ 0.7248451  -0.13313441]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "False\n",
      "tf.Tensor([[ 0.709961   -0.12432492]], shape=(1, 2), dtype=float32)\n",
      "1\n",
      "False\n",
      "tf.Tensor([[ 0.7282995  -0.13617238]], shape=(1, 2), dtype=float32)\n",
      "1\n",
      "False\n",
      "tf.Tensor([[ 0.7128823  -0.12740602]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "False\n",
      "tf.Tensor([[ 0.69647884 -0.12115029]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "False\n",
      "tf.Tensor([[ 0.68073994 -0.11564522]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "False\n",
      "tf.Tensor([[ 0.6621347  -0.10990647]], shape=(1, 2), dtype=float32)\n",
      "1\n",
      "False\n",
      "tf.Tensor([[ 0.6445516  -0.10277456]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "False\n",
      "tf.Tensor([[ 0.6335287  -0.10267139]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "True\n",
      "tf.Tensor([[ 0.60743403 -0.01165123]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "True\n",
      "tf.Tensor([[0.58858395 0.07082333]], shape=(1, 2), dtype=float32)\n",
      "1\n",
      "True\n",
      "tf.Tensor([[0.5693424  0.15010832]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "True\n",
      "tf.Tensor([[0.5584068  0.22406638]], shape=(1, 2), dtype=float32)\n",
      "1\n",
      "True\n",
      "tf.Tensor([[0.5512043  0.28570038]], shape=(1, 2), dtype=float32)\n",
      "1\n",
      "True\n",
      "tf.Tensor([[0.5459877 0.342847 ]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "True\n",
      "tf.Tensor([[0.5411493  0.40004453]], shape=(1, 2), dtype=float32)\n",
      "1\n",
      "True\n",
      "tf.Tensor([[0.53215945 0.4557299 ]], shape=(1, 2), dtype=float32)\n",
      "1\n",
      "True\n",
      "tf.Tensor([[0.5243215  0.50341576]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "True\n",
      "tf.Tensor([[0.51834995 0.52985585]], shape=(1, 2), dtype=float32)\n",
      "1\n",
      "True\n",
      "tf.Tensor([[0.5218989 0.5679715]], shape=(1, 2), dtype=float32)\n",
      "1\n",
      "True\n",
      "tf.Tensor([[0.52091235 0.59792346]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "True\n",
      "tf.Tensor([[0.51205695 0.61925584]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "True\n",
      "tf.Tensor([[0.5011476  0.63335663]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "True\n",
      "tf.Tensor([[0.4854387 0.6396373]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "True\n",
      "tf.Tensor([[0.46645966 0.6394328 ]], shape=(1, 2), dtype=float32)\n",
      "1\n",
      "True\n",
      "tf.Tensor([[0.7352335  0.81019455]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "True\n",
      "tf.Tensor([[0.708257   0.79887396]], shape=(1, 2), dtype=float32)\n",
      "1\n",
      "True\n",
      "tf.Tensor([[0.7272435  0.81178266]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "True\n",
      "tf.Tensor([[0.7026134 0.7895473]], shape=(1, 2), dtype=float32)\n",
      "1\n",
      "True\n",
      "tf.Tensor([[0.72548664 0.7936483 ]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "True\n",
      "tf.Tensor([[0.7523055 0.6726734]], shape=(1, 2), dtype=float32)\n",
      "1\n",
      "True\n",
      "tf.Tensor([[0.72769684 0.65287113]], shape=(1, 2), dtype=float32)\n",
      "1\n",
      "True\n",
      "tf.Tensor([[0.70706785 0.63225675]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "True\n",
      "tf.Tensor([[0.68901587 0.60784954]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "True\n",
      "tf.Tensor([[0.67230856 0.5802048 ]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "True\n",
      "tf.Tensor([[0.6514355  0.55298734]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "True\n",
      "tf.Tensor([[0.6303226 0.5265518]], shape=(1, 2), dtype=float32)\n",
      "1\n",
      "True\n",
      "tf.Tensor([[0.6075246 0.5013773]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "True\n",
      "tf.Tensor([[0.57911456 0.48046106]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "True\n",
      "tf.Tensor([[0.548002   0.46157676]], shape=(1, 2), dtype=float32)\n",
      "1\n",
      "True\n",
      "tf.Tensor([[0.5145815  0.44596657]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "True\n",
      "tf.Tensor([[0.48374447 0.4363636 ]], shape=(1, 2), dtype=float32)\n",
      "1\n",
      "True\n",
      "tf.Tensor([[0.4605733 0.4278299]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "True\n",
      "tf.Tensor([[0.4357934 0.4185882]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "True\n",
      "Episode 1 | Total point average of the last 100 episodes: 10.00tf.Tensor([[0.6719009 0.6878962]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "True\n",
      "tf.Tensor([[0.6628643 0.6728257]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "True\n",
      "tf.Tensor([[0.6553013  0.65888983]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "True\n",
      "tf.Tensor([[0.64702773 0.6482306 ]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "True\n",
      "tf.Tensor([[0.63909537 0.63821626]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "True\n",
      "tf.Tensor([[0.6313074 0.628747 ]], shape=(1, 2), dtype=float32)\n",
      "1\n",
      "True\n",
      "tf.Tensor([[0.68659616 0.7157357 ]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "True\n",
      "tf.Tensor([[0.68188524 0.70669025]], shape=(1, 2), dtype=float32)\n",
      "1\n",
      "True\n",
      "tf.Tensor([[0.697633  0.7327596]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "True\n",
      "tf.Tensor([[0.6921616 0.7242247]], shape=(1, 2), dtype=float32)\n",
      "1\n",
      "True\n",
      "tf.Tensor([[0.70796114 0.7495279 ]], shape=(1, 2), dtype=float32)\n",
      "1\n",
      "True\n",
      "tf.Tensor([[0.71263474 0.757215  ]], shape=(1, 2), dtype=float32)\n",
      "1\n",
      "True\n",
      "tf.Tensor([[0.7163142  0.76448506]], shape=(1, 2), dtype=float32)\n",
      "1\n",
      "True\n",
      "tf.Tensor([[0.719978   0.77130055]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "True\n",
      "tf.Tensor([[0.71217966 0.7606383 ]], shape=(1, 2), dtype=float32)\n",
      "0\n",
      "True\n",
      "tf.Tensor([[0.7042606 0.7483134]], shape=(1, 2), dtype=float32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 2440, in predict_function  *\n        return step_function(self, iterator)\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 2425, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 2413, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 2381, in predict_step\n        return self(x, training=False)\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/input_spec.py\", line 253, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential' (type Sequential).\n    \n    Input 0 of layer \"dense\" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (None,)\n    \n    Call arguments received by layer 'sequential' (type Sequential):\n      • inputs=tf.Tensor(shape=(None,), dtype=float32)\n      • training=False\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m q_values \u001b[38;5;241m=\u001b[39m q_network(state_qn)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(q_values)\n\u001b[0;32m---> 31\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(action)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Take action A and receive reward R and the next state S'\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 18\u001b[0m, in \u001b[0;36mget_action\u001b[0;34m(model, state, num_actions, epsilon)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(num_actions)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 18\u001b[0m     q_values \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39margmax(q_values[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/n1/j4qhhn297tj4tnrpbsvg4q800000gn/T/__autograph_generated_filem534uimt.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 2440, in predict_function  *\n        return step_function(self, iterator)\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 2425, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 2413, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 2381, in predict_step\n        return self(x, training=False)\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/input_spec.py\", line 253, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential' (type Sequential).\n    \n    Input 0 of layer \"dense\" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (None,)\n    \n    Call arguments received by layer 'sequential' (type Sequential):\n      • inputs=tf.Tensor(shape=(None,), dtype=float32)\n      • training=False\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "num_episodes = 2000\n",
    "max_num_timesteps = 1000\n",
    "\n",
    "total_point_history = []\n",
    "\n",
    "num_p_av = 100    # number of total points to use for averaging\n",
    "epsilon = 1.0     # initial ε value for ε-greedy policy\n",
    "\n",
    "# Create a memory buffer D with capacity N\n",
    "memory_buffer = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "# Set the target network weights equal to the Q-Network weights\n",
    "target_q_network.set_weights(q_network.get_weights())\n",
    "\n",
    "\n",
    "for i in range(20):\n",
    "    \n",
    "    # Reset the environment to the initial state and get the initial state\n",
    "    state = env.reset()[0]\n",
    "    total_points = env.reset()[1]['score']\n",
    "\n",
    "    # max_num_timesteps = 1000. This means that the episode will automatically terminate if the episode hasn't terminated after 1000 time steps.\n",
    "    for t in range(100):\n",
    "    \n",
    "        # From the current state S choose an action A using an ε-greedy policy\n",
    "        state_qn = np.expand_dims(state, axis=0)  # state needs to be the right shape for the q_network\n",
    "        q_values = q_network(state_qn)\n",
    "        print(q_values)\n",
    "        action = get_action(q_network, state, num_actions, epsilon)\n",
    "        print(action)\n",
    "         \n",
    "        # Take action A and receive reward R and the next state S'\n",
    "        next_state, reward, done, _, current_score = env.step(action)\n",
    "        \n",
    "        # Store experience tuple (S,A,R,S') in the memory buffer.\n",
    "        # We store the done variable as well for convenience.\n",
    "        memory_buffer.append(experience(state, action, reward, next_state, done))\n",
    "        \n",
    "        # Only update the network every NUM_STEPS_FOR_UPDATE time steps.\n",
    "        update = check_update_conditions(t, NUM_STEPS_FOR_UPDATE, memory_buffer)\n",
    "        print(update)\n",
    "        if update:\n",
    "            # Sample random mini-batch of experience tuples (S,A,R,S') from D\n",
    "            experiences = get_experiences(memory_buffer)\n",
    "            \n",
    "            # Set the y targets, perform a gradient descent step,\n",
    "            # and update the network weights.\n",
    "            agent_learn(experiences, GAMMA)\n",
    "\n",
    "        state = next_state.copy()\n",
    "        total_points += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    total_point_history.append(total_points)\n",
    "    av_latest_points = np.mean(total_point_history[-num_p_av:])\n",
    "\n",
    "    # Update the ε value\n",
    "    epsilon = get_new_eps(epsilon)\n",
    "\n",
    "    print(f\"\\rEpisode {i+1} | Total point average of the last {num_p_av} episodes: {av_latest_points:.2f}\", end=\"\")\n",
    "\n",
    "    if (i+1) % num_p_av == 0:\n",
    "        print(f\"\\rEpisode {i+1} | Total point average of the last {num_p_av} episodes: {av_latest_points:.2f}\")\n",
    "\n",
    "    # We will consider that the environment is solved if we get an\n",
    "    # average of 200 points in the last 100 episodes.\n",
    "    if av_latest_points >= 10000:\n",
    "        print(f\"\\n\\nEnvironment solved in {i+1} episodes!\")\n",
    "        q_network.save('flappy_bird.h5')\n",
    "        break\n",
    "\n",
    "tot_time = time.time() - start\n",
    "print(f\"\\nTotal Runtime: {tot_time:.2f} s ({(tot_time/60):.2f} min)\")       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81daa280-3fb7-4f99-8c1a-ffa398aa3a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15612033-30d2-4c04-a8a1-a906208f2d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# import gym\n",
    "\n",
    "# # Create Flappy Bird environment (you may need to install the gym-flappy-bird package)\n",
    "# env = gymnasium.make('FlappyBird-v0')\n",
    "\n",
    "# # Define constants\n",
    "# state_size = 12  # Adjust based on your state representation\n",
    "# action_size = env.action_space.n\n",
    "# learning_rate = 0.001\n",
    "# gamma = 0.95\n",
    "# epsilon = 1.0\n",
    "# epsilon_decay = 0.995\n",
    "# min_epsilon = 0.01\n",
    "# batch_size = 32\n",
    "# memory_size = 100000\n",
    "\n",
    "# # Build the Deep Q-Learning model\n",
    "# model = Sequential()\n",
    "# model.add(Dense(24, input_dim=state_size, activation='relu'))\n",
    "# model.add(Dense(24, activation='relu'))\n",
    "# model.add(Dense(action_size, activation='linear'))\n",
    "# model.compile(loss='mse', optimizer=Adam(lr=learning_rate))\n",
    "\n",
    "# # Experience replay buffer\n",
    "# class ReplayBuffer:\n",
    "#     def __init__(self, size):\n",
    "#         self.memory = []\n",
    "#         self.size = size\n",
    "\n",
    "#     def add(self, experience):\n",
    "#         self.memory.append(experience)\n",
    "#         if len(self.memory) > self.size:\n",
    "#             self.memory.pop(0)\n",
    "\n",
    "#     def sample(self, batch_size):\n",
    "#         if len(self.memory) == 0:\n",
    "#             return []\n",
    "#         else:\n",
    "#             indices = np.random.choice(len(self.memory), batch_size, replace=True)\n",
    "#             return [self.memory[i] for i in indices]\n",
    "\n",
    "# replay_buffer = ReplayBuffer(memory_size)\n",
    "\n",
    "# # Epsilon-greedy strategy\n",
    "# def epsilon_greedy(model, state, epsilon):\n",
    "#     if np.random.rand() <= epsilon:\n",
    "#         return np.random.choice(action_size)\n",
    "#     else:\n",
    "#         q_values = model.predict(state)\n",
    "#         return np.argmax(q_values[0])\n",
    "\n",
    "# # Update Q-values using experience replay\n",
    "# def update_q_values(model, replay_buffer, batch_size, gamma):\n",
    "#     minibatch = replay_buffer.sample(batch_size)\n",
    "#     for state, action, reward, next_state, done in minibatch:\n",
    "#         target = reward\n",
    "#         if not done:\n",
    "#             target = reward + gamma * np.amax(model.predict(next_state)[0])\n",
    "#         q_values = model.predict(state)\n",
    "#         q_values[0][action] = target\n",
    "#         model.fit(state, q_values, epochs=1, verbose=0)\n",
    "\n",
    "# # Deep Q-Learning algorithm\n",
    "# def deep_q_learning(model, env, episodes):\n",
    "#     global epsilon\n",
    "#     for episode in range(episodes):\n",
    "#         state = env.reset()[0]\n",
    "#         state = np.reshape(state, [1, state_size])\n",
    "\n",
    "#         for time in range(500):  # Adjust the maximum number of time steps as needed\n",
    "#             # Choose an action using epsilon-greedy strategy\n",
    "#             action = epsilon_greedy(model, state, epsilon)\n",
    "\n",
    "#             # Take the chosen action and observe the next state and reward\n",
    "#             next_state, reward, done, _, score = env.step(action)\n",
    "#             next_state = np.reshape(next_state, [1, state_size])\n",
    "\n",
    "#             # Store the experience in the replay buffer\n",
    "#             replay_buffer.add((state, action, reward, next_state, done))\n",
    "\n",
    "#             # Update the Q-values using experience replay\n",
    "#             update_q_values(model, replay_buffer, batch_size, gamma)\n",
    "\n",
    "#             # Update the current state\n",
    "#             state = next_state\n",
    "\n",
    "#             if done:\n",
    "#                 break\n",
    "\n",
    "#         # Decay epsilon\n",
    "#         epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "\n",
    "# # Train the model\n",
    "# episodes = 1000  # Adjust the number of episodes as needed\n",
    "# deep_q_learning(model, env, episodes)\n",
    "\n",
    "# # Test the trained model\n",
    "# total_reward = 0\n",
    "# state = env.reset()\n",
    "# state = np.reshape(state, [1, state_size])\n",
    "\n",
    "# for _ in range(500):  # Adjust the maximum number of time steps as needed\n",
    "#     action = np.argmax(model.predict(state)[0])\n",
    "#     next_state, reward, done, _ = env.step(action)\n",
    "#     total_reward += reward\n",
    "#     next_state = np.reshape(next_state, [1, state_size])\n",
    "#     state = next_state\n",
    "\n",
    "#     if done:\n",
    "#         break\n",
    "\n",
    "# print(\"Total Reward:\", total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cefb5b-7353-462d-9970-eb521a294cb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
